{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1u_26xLI-EN1yNd5i-hE_y4Gubply-sQ8","timestamp":1757344091748}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Tokenization Techniques in NLP\n","Week 2 NLP Pipeline\n","PBA/ Genap 2025/ Irmasari Hafidz\n","irma@its.ac.id\n"],"metadata":{"id":"fNnKUG6g7ehT"}},{"cell_type":"code","source":["# ================================\n","# STEP 4 - TOKENIZATION\n","# ================================\n","\n","import pandas as pd\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","nltk.download('punkt_tab')  # penting biar ga error di Colab baru\n","\n","# Keras tokenizer\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# --- 1) Load data (hasil stemming) ---\n","df = pd.read_csv(\"emirates_reviews_stemmed.csv\")\n","print(\"Kolom tersedia:\", df.columns.tolist())\n","\n","# gunakan kolom hasil stemming, fallback ke review_after_stopwords kalau belum ada\n","TEXT_COL = \"stemmed_porter\" if \"stemmed_porter\" in df.columns else \"review_after_stopwords\"\n","print(f\"[INFO] Tokenization source column: {TEXT_COL}\")\n","\n","# --- 2) Tokenization dengan NLTK ---\n","df[\"tokens\"] = df[TEXT_COL].fillna(\"\").astype(str).apply(word_tokenize)\n","print(df[[\"tokens\"]].head())\n","\n","# --- 3) Statistik sederhana ---\n","from collections import Counter\n","all_tokens = [w for row in df[\"tokens\"] for w in row]\n","freq = Counter(all_tokens)\n","print(\"Top 10 token:\", freq.most_common(10))\n","\n","# --- 4) Tokenizer Keras ---\n","texts = df[TEXT_COL].fillna(\"\").astype(str).tolist()\n","tokenizer = Tokenizer(num_words=None, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(texts)\n","\n","# ubah teks jadi sequence angka\n","sequences = tokenizer.texts_to_sequences(texts)\n","print(\"Contoh sequence:\", sequences[:3])\n","\n","# padding (supaya semua review punya panjang sama, misalnya 100 token)\n","max_len = 100\n","padded = pad_sequences(sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n","print(\"Shape padded sequences:\", padded.shape)\n","\n","# --- 5) Simpan hasil ---\n","df.to_csv(\"emirates_reviews_tokenized.csv\", index=False)\n","print(\"[OK] Saved emirates_reviews_tokenized.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yh9QUC1C4_AE","executionInfo":{"status":"ok","timestamp":1757344711990,"user_tz":-420,"elapsed":1594,"user":{"displayName":"Maureen Fadhliphya","userId":"15534252903059071134"}},"outputId":"cddf73ee-b644-4fa2-dfc2-17f6873e1a11"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Kolom tersedia: ['content', 'normalized', 'review_after_stopwords', 'wordCount', 'wordCount_after_stopwords', 'stemmed_porter', 'stemmed_lancaster']\n","[INFO] Tokenization source column: stemmed_porter\n","                     tokens\n","0     [unabl, book, ticket]\n","1            [beauti, love]\n","2          [easi, use, use]\n","3                 [fantast]\n","4  [best, easi, use, navig]\n","Top 10 token: [('easi', 4025), ('use', 3508), ('good', 3071), ('great', 1785), ('excel', 1740), ('best', 1337), ('servic', 1137), ('book', 995), ('work', 972), ('friendli', 948)]\n","Contoh sequence: [[102, 9, 45], [346, 12], [2, 3, 3]]\n","Shape padded sequences: (20681, 100)\n","[OK] Saved emirates_reviews_tokenized.csv\n"]}]}]}